{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Ex 06 - Classification (Naïve Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27.03.2022, Lukas Kretschmar (lukas.kretschmar@ost.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have some fun with Classification and Model Evaluation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are going to get an introduction to classification and how you can evaluate and visualize the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need some data to run our classification algorithm on.\n",
    "The scikit-learn package (`sklearn`), which we will also use for the algorithms and evaluation, offers some sample datasets that we can use to work with.\n",
    "The following lines download articles/messages and their assigned categories.\n",
    "\n",
    "Please note: The data will be downloaded the first time. Thus, the execution will take some time during the first run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following calls, we get a glimps on the structure of our demo data and what's in there.\n",
    "\n",
    "`keys()` returns the root level keys of the data object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use one of these keys to access parts of the demo data that are relevant to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"data\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array from above contains the index of the corresponding label that the data belongs to.\n",
    "The cleartext names of these categories are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"target_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the example from above belongs to the following category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataId = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"data\"][dataId]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target/Category/Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catId = data[\"target\"][dataId]\n",
    "data[\"target_names\"][catId]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the articles contain text, we need some preparation of the data so the Naïve Bayes algorithm can \"understand\" (read: work with) it.\n",
    "In case of the text, we will use the [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) algorithm to assign a significance to every word within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train and test the algorithm, we need two distinct sets of data.\n",
    "Additionally, within this introduction, we'll just use a subset of all categories which are defined in the `categories` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['sci.crypt', 'sci.electronics',  'sci.med',  'sci.space',\n",
    " 'soc.religion.christian',\n",
    " 'talk.politics.guns',  'talk.politics.mideast',  'talk.politics.misc',  'talk.religion.misc']\n",
    "train = fetch_20newsgroups(subset=\"train\", categories=categories)\n",
    "test = fetch_20newsgroups(subset=\"test\", categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the `subset` parameter will already split the data into two sets.\n",
    "Later, you'll see another function how you can do the same with any dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model contains two steps:\n",
    "1. Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "train_mod = tfidf.fit_transform(train[\"data\"])\n",
    "train_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't go into detail about [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf).\n",
    "\n",
    "In short: TF-IDF stands for *term frequency - inverse document frequency*.\n",
    "The algorithm takes every word of a document, counts how many times it's in there compared to all words (*term frequency* part), and couples its significance to the occurrence in all documents (*inverse document frequency*).\n",
    "The idea is, that if a word is used many times in a document, but seldom in the others, it's important to find similar documents.\n",
    "And if a word is quite frequent in all documents, it's not that useful for finding similar documents.\n",
    "\n",
    "If you are interested, feel free to dig deeper.\n",
    "But for this course, going deeper is not relevant and is thus out of scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Actually training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(train_mod, train[\"target\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a trained model of our Naïve Bayes algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can do predictions (use the model), we also have to process the testing data by running it through the `TfidfVectorizer()`.\n",
    "And then predictions can be made by calling the `predict()` method on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod = tfidf.transform(test[\"data\"]) \n",
    "pred = model.predict(test_mod)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it.\n",
    "As you see, we need to call a `fit()` method to train the model and with a call to `predict()` we can get some predictions for another dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as you also saw, we had to do some preprocessing (vectorizing) before we could build the model.\n",
    "These steps are usually the same for every approach:\n",
    "- Preprocessing the train data\n",
    "- Train the model\n",
    "- Preprocessing the test data\n",
    "- Use test data to make predictions\n",
    "\n",
    "To avoid code, and for that matter, errors in code, scikit-learn introduced a concept called *Feature Pipelines*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the algorithms included in sklearn always provide the same interface (there is always a `fit()` and `predict()` method), we just have to provide and configure the algorithms.\n",
    "The calls to the methods are made by a `Pipeline` object.\n",
    "The code from above can be re-written as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train[\"data\"], train[\"target\"])\n",
    "pred = model.predict(test[\"data\"])\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the code looks much cleaner and clearer, but it's the same as in the lines above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having our predictions, we can go on and evaluate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we can simple check how well our predictions match the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means, that our predictions work nearly 80% of the time - and as you will see later on, this number is prabably higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, a simple score isn't enough to determine if a model works well with our data.\n",
    "We want to evaluate the model on our training data as well - before we even make predictions.\n",
    "In this case, we can use crossvalidation to check multiple combinations of our data against the model and see how well each part performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using crossvalidation, we have to provide a model (in this case our pipeline object), the dataset split into data and categories.\n",
    "And with the `cv` parameter, we specify how many sets for crossvalidation should be built (in this case 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossVal = cross_val_score(model, train[\"data\"], train[\"target\"], cv=5)\n",
    "print(f\"{crossVal} -> {crossVal.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on our training data, we can see that our model is 86.5% correct with its predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since crossvalidation only trains our model with parts of the train set, we need to retrain our model with the full train set at the end (before we start actually using the model).\n",
    "And to be sure we won't use any legacy settings, we recreate the model as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "model.fit(train[\"data\"], train[\"target\"])\n",
    "pred = model.predict(test[\"data\"])\n",
    "accuracy_score(test[\"target\"], pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize the performance of a model is to plot a confusion matrix.\n",
    "sklearn offers a method to create this confusion matrix, and using `seaborn` we can plot it nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(test[\"target\"], pred)   # Comparing the expected categories with the predicted categories\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(matrix.T, square=True, annot=True, fmt=\"d\", cbar=True, xticklabels=train.target_names, yticklabels=train.target_names, ax=ax)\n",
    "ax.set(xlabel=\"True Category\", ylabel=\"Predicted Category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, the algorithm performed really well (brigth diagonal line), except it got confused by *soc.religion.christion* and *talk.religion.misc*.\n",
    "But I'd say in this case, this confusion isn't that severe as religion is the overall topic.\n",
    "So it wasn't that wrong.\n",
    "The same is with *talk.politics.guns* and *talk.politics.misc* or *sci.crypt* and *sci.electronics*.\n",
    "It points in the right direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`confusion_matrix()` can also be used to reorder and sample the used labels.\n",
    "If we just want to focus on the `sci` labels, and in the reversed order, we can set the `labels` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelIdx = [3,2,1,0]\n",
    "labels = np.array(train[\"target_names\"])[labelIdx]\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, where the labels are indices pointing to the actual name of the label, we have to split the information into `labelIdx` (filtering in `confusion_matrix()`) and `labels` (ticks in the visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(test[\"target\"], pred, labels=labelIdx)   # Comparing the expected categories with the predicted categories\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "sns.heatmap(matrix.T, square=True, annot=True, fmt=\"d\", cbar=True, xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "ax.set(xlabel=\"True Category\", ylabel=\"Predicted Category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we saw that the model performed pretty good, we can use it for our own input.\n",
    "To simplify the usage, we write a simple method, that takes away writting the same lines of code all the time.\n",
    "The method takes our text, the model and categories, predicts the category (`p` contains the index of the category) and we will use this information to get the name of the category back.\n",
    "\n",
    "*Please note:* This is only necessary for this model since the categories are encoded as numbers in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_category(text, model=model, categories=train[\"target_names\"]):\n",
    "    p = model.predict([text])   # We need to insert the text as part of a list\n",
    "    return categories[p[0]]     # The predicted category is also a list, and since we only provided one text, there is only one category predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category(\"I believe in god\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category(\"Fly me to the moon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category(\"How do hash algorithms work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And sometimes, based on the limited data, and probably the actuallity of the dataset, the results can a bit funny."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category(\"Android vs iOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category(\"Data science is fun. Hard to learn, but if you master it, your life gets easier.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category(\"Coronavirus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_category(\"Corona virus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may noticed that the longer the input, the better the prediction will be since the model will have more input to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the demo data from the beginning, we do not always (read: never) have the luxury that the data already comes with a train and test set.\n",
    "In this case, we need to split our data into a test set and training set.\n",
    "\n",
    "Since this is a common requirement in Data Science, there is a method we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as an example, we will use our `train` data to be split into a subset for training `X_train` (data) and `y_train` (categories) and one for testing `X_test` (data) and `y_test` (categories).\n",
    "With `train_size` we provide the percentage of data used as part of the training set, the rest will be part of the test set (there is also a `test_size` that you could use instead).\n",
    "And `random_state` ensures that we always split the data the same way (for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train[\"data\"], train[\"target\"], train_size=.75, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this example simply took 75% of the original data as train set.\n",
    "And the other 25% are in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn offers some more Naïve Bayes algorithms that could be interesting for you (list not exhaustive):\n",
    "- **MultinomialNB:** For features that have a number of occurrences (how many times? Used above)\n",
    "- **BernoulliNB:** If we just say that a feature is present or not (`0` or `1` | `True` or `False`)\n",
    "- **GaussianNB:** If the features have a gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex01 - News Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the introduction, you saw the usage of the *20newsgroups* dataset with some categories.\n",
    "Now, create and train a model using the other categories (*alt.atheism* to *rec.sport.hockey*) and do some predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, specify the categories you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, fetch the `train` and `test` sets for these categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the pipeline using the `MultinomialNB` classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the categories for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the heatmap of the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the categories for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"General Motors is a car manufacturer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"The Boston Red Sox actually wear red socks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Have you tried turning it off and on again? Maybe a reboot helps.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex06_01_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex02 - Spam Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to train a simple spam filter.\n",
    "First, load the data from **Ex06_02_Data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the column *Label* contains the information if a message was spam (`spam`) or not spam (`ham`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and test sets.\n",
    "The train set should contain 70% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create, train and predict the labels using the same approach as shown in the introduction.\n",
    "But this time, we will set a parameter to the `MultinomialNB()` classifier - set `alpha=.1`.\n",
    "Your constructor call should look like `MultinomialNB(alpha=.1)`.\n",
    "This makes the classifier a bit more radical in deciding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix as heatmap to see how well the classifier performs.\n",
    "Entries labeled as `spam` should be shown in the top left corner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, use the model to predict:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Whazaaaap!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Congratulations, you've won the lottery.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Sorry, I'll be late.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"I'm a nigerian prince who needs to transfer some gold. You can have $1'000'000 if you work with me.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, your model was nearly always correct - except for the last one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex06_02_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex03 - Alexa Reviews (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will try to predict if a customer liked a product or not.\n",
    "We will do this by analysing the reviews of the Amazon Alexa.\n",
    "As you will see, the exercise is split into two parts since it's a bit bigger than the previous ones.\n",
    "\n",
    "*Please note:* If you can't complete this part, you can also start with the next exercise.\n",
    "The idea is that you take your data from this exercise to the next one, but we've also provided the input data for the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file **Ex06_03_Data.csv** and show the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have *verified_reviews* that we will use to predict the *feedback*.\n",
    "Here, `1` stands for like, and `0` for dislike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and test sets with 80% of the data in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model, the same way you've done it before (here, we won't use `alpha`).\n",
    "And show the crossvalidation score with `cv=5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the model performs really well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, train the model with your train set and predict the feedback for your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the result as confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you now see, your model performed really well, because it just assumed everyone liked Alexa.\n",
    "Conspiracy theorist probalby think that Alexa and scikit-learn are working together - but that's not the case - well, probably."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the amount of positive and negative feedbacks.\n",
    "\n",
    "*Hint:* Use `value_counts()` on the column and the rest is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have a severe case of class imbalance.\n",
    "And the model just figured that dislikes are so rare, that it's easier just to assume all reviews are positive (or at least nearly all the time)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix this, we will upsample the negative reviews.\n",
    "This means, we just duplicate reviews until the imbalance isn't a problem anymore.\n",
    "To do so, we use a method called `resample()`.\n",
    "This method takes 4 parameters:\n",
    "- `arrays`: The data to take samples from\n",
    "- `replace`: Boolean parameter to state if duplicates are allowed\n",
    "- `n_samples`: Number of samples to take from the `array`\n",
    "- `random_state`: Seed to reproduce the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsample the negative reviews to 2/3 of positive reviews.\n",
    "And build a new `DataFrame` containing the positive reviews and your newly upsampled negative reviews.\n",
    "\n",
    "*Hint:* You need `pd.concat()` to create the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the *feedback* again to show that the imbalance is gone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex06_03_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex04 - Alexa Reviews (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will actually do the Amazon Alexa like/dislike classifier.\n",
    "You can take the data from the previous exercise, or you can load **Ex06_04_Data.csv**.\n",
    "This file should contain roughly the same data as you created in the exercise above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, create your train and test sets (choose your own split ratio), create a model (`alpha=.1`) and show the crossvalidation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model performs equally well as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model (again with `alpha=.1`), train it and predict the feedback for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the confusion matrix to show that the classifier now performs better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the classifier works now.\n",
    "At least negative reviews are found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now predict the feedback for the following reviews:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"I love my Alexa!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"I hate it!!!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"It does not work. Sound quality is bad.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"It's a cool tool. My life got way easier.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Just the works product ever\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"The NSA is probably listening...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it performs quite well.\n",
    "I'm just not quite sure about the last one..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex06_04_Sol.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
