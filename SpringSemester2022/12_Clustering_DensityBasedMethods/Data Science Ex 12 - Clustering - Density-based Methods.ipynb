{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Ex 12 - Clustering (Density-based Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15.05.2022, Lukas Kretschmar (lukas.kretschmar@ost.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have some Fun with Density-based Clustering approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to have a look at density-based clustering.\n",
    "Further, we have a look at possibilities to scale and normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go to the density-based clustering approach, we want to introduce some preprocessing steps for a data scientist.\n",
    "\n",
    "- Scaling numerical data\n",
    "- Normalizing numerical data\n",
    "- Encoding categorical data\n",
    "\n",
    "This knowledge is important in general, but in this exercise especially since we need to prepare the data for clustering.\n",
    "For one, it could be a good idea to reduce the dimensions (number of features) of our data and therefore improve the runtime of our algorithms.\n",
    "Or we can visualize the data easier.\n",
    "Further, since clustering needs to calculate the distance between points, the values should be in the same range.\n",
    "Otherwise, some features will dominate over others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When building clusters, it's essential that the values are in a comparable range.\n",
    "Otherwise, calculating distances will include biases (higher values have a larger impact - e.g. one column is in `km`, another in `mm`).\n",
    "\n",
    "Therefore, we need to know some techniques to scale our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: https://scikit-learn.org/stable/modules/preprocessing.html & https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a look at the follwing possibilities (but there are many more, this list is not exhaustive):\n",
    "- **Normalizer**: Normalizes a row to unit norm (the sum of all values is `1`, the values are relative to each other).\n",
    "- **MinMaxScaler**: Transforms features into a defined range.\n",
    "- **RobustScaler**: Scales features but mitigates outliers by scaling all values into a given quantile range.\n",
    "- **StandardScaler**: Scales features to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer, MinMaxScaler, RobustScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "data = pd.DataFrame(rng.randn(100000) + 5, columns=[\"Values\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample data has a normal distribution around a mean of 5.\n",
    "\n",
    "Now, let's have a look what effects the scalers and normalizers have on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Normalizer()` scales the values in a way that the sum is `1`.\n",
    "We already saw this method in action in the last exercise - when we called `normalize()`.\n",
    "`Normalize()` is just the class implementing the method and can be used in `Pipelines` or when instead of a method a class is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer()\n",
    "data_norm = norm.fit_transform(data.T) # Since the algorithm works on rows, we have to transform the data\n",
    "data_norm = pd.DataFrame(data_norm.T)\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "data_norm.hist(bins=100, ax=ax[0])\n",
    "ax[0].set(title=\"Histogram of Normalizer (l2, default)\")\n",
    "\n",
    "(data_norm[0]\n",
    "     .sort_values()             # Sorting all values in ascending order\n",
    "     .reset_index(drop=True)    # Removing index\n",
    "     .apply(lambda v: v**2)     # Squaring values\n",
    "     .cumsum()                  # Taking the cumulative sum\n",
    "     .plot(ax=ax[1]))           # Plotting the line\n",
    "ax[1].set(title=\"Cumulative sum of normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to square each value since the default behavior of `Normalize()` uses squares when normalizing.\n",
    "If this behavior is not needed, but just the values relative to each other, we can set the `norm` parameter to `l1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = Normalizer(norm=\"l1\")\n",
    "data_norm = norm.fit_transform(data.T)\n",
    "data_norm = pd.DataFrame(data_norm.T)\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "data_norm.hist(bins=100, ax=ax[0])\n",
    "ax[0].set(title=\"Histogram of Normalizer (l1)\")\n",
    "\n",
    "(data_norm[0]\n",
    "    .sort_values()          # Sorting all values in ascending order\n",
    "    .reset_index(drop=True) # Removing index\n",
    "    .cumsum()               # Taking the cumulative sum\n",
    "    .plot(ax=ax[1]))         # Plotting the line\n",
    "ax[1].set(title=\"Cumulative sum of normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the values changed but the sum is still `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MinMaxScaler` transforms the data so it is in a given range.\n",
    "The default range is `(0,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler()\n",
    "data_minmax = minmax.fit_transform(data)\n",
    "data_minmax = pd.DataFrame(data_minmax)\n",
    "print(f\"Min: {data_minmax.min()[0]}\")\n",
    "print(f\"Max: {data_minmax.max()[0]}\")\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "data_minmax.hist(bins=100, ax=ax)\n",
    "ax.set(title=\"Histogram of MinMaxScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all the values are now scaled to a range from `0` to `1`.\n",
    "We can change this by providing a range to `feature_range`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax = MinMaxScaler(feature_range=(2,4))\n",
    "data_minmax = minmax.fit_transform(data)\n",
    "data_minmax = pd.DataFrame(data_minmax)\n",
    "print(f\"Min: {data_minmax.min()[0]}\")\n",
    "print(f\"Max: {data_minmax.max()[0]}\")\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "data_minmax.hist(bins=100, ax=ax)\n",
    "ax.set(title=\"Histogram of MinMaxScaler (2,4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RobustScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RobustScaler` transforms the data based on a given quantile range (default is 1st quartile (25%) to 3rd quartile (75%)).\n",
    "With this approach we try to remove the impact of outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust = RobustScaler()\n",
    "data_robust = robust.fit_transform(data)\n",
    "data_robust = pd.DataFrame(data_robust)\n",
    "fig, ax = plt.subplots(1,2,figsize=(20, 5))\n",
    "\n",
    "(data - 5).hist(bins=100, ax=ax[0])\n",
    "ax[0].set(title=\"Original Data (shifted to 0)\")\n",
    "\n",
    "data_robust.hist(bins=100, ax=ax[1])\n",
    "ax[1].set(title=\"Data with RobustScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the scaled data has a smaller range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `StandardScaler` removes the mean and transforms the data using the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = StandardScaler()\n",
    "data_std = std.fit_transform(data)\n",
    "data_std = pd.DataFrame(data_std)\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "data.hist(bins=100, ax=ax[0])\n",
    "ax[0].set(title=\"Original Data\")\n",
    "\n",
    "data_std.hist(bins=100, ax=ax[1])\n",
    "ax[1].set(title=\"Data with StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(42)\n",
    "data_exp = pd.DataFrame(rng.uniform(5,10,size=100000), columns=[\"Values\"])\n",
    "data_exp_std = StandardScaler().fit_transform(data_exp)\n",
    "data_exp_std = pd.DataFrame(data_exp_std)\n",
    "fig, ax = plt.subplots(1,2,figsize=(20,5))\n",
    "\n",
    "data_exp.hist(bins=100, ax=ax[0])\n",
    "ax[0].set(title=\"Original Data (Uniform Distribution)\")\n",
    "\n",
    "data_exp_std.hist(bins=100, ax=ax[1])\n",
    "ax[1].set(title=\"Data with StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the scaled data has now a mean of `0` but the distribution hasn't changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with categorial data, we usually need to transform it into numbers.\n",
    "We have already seen one approach with the `pd.get_dummies()` method.\n",
    "Here, we will introduce another method that accomplishes the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = [\"Engineer\", \"Accountant\", \"Manager\", \"Professor\", \"Student\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = pd.DataFrame({\"Name\" : [\"Johnny\", \"Jenny\", \"Jake\"], \"Job\":[\"Engineer\", \"Manager\", \"Student\"]})\n",
    "people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False) # sparse=False will get us an array as result and not a sparse array object\n",
    "\n",
    "data_cat = enc.fit_transform(people[\"Job\"].to_numpy().reshape(-1,1))\n",
    "print(f\"Raw data:\\r\\n{data_cat}\")\n",
    "print()\n",
    "\n",
    "data_cat = pd.DataFrame(data_cat, columns=[\"Is_\" + str(c) for c in enc.categories_[0]])\n",
    "data_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the result is a dataset with multiple columns containing a `1` if a category was present in a row.\n",
    "Or `0` to indicate the absence of this category.\n",
    "We can also provide a complete list of all possible values.\n",
    "And then for every possibility a column is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(categories=[jobs], sparse=False)\n",
    "data_cat = enc.fit_transform(people[\"Job\"].to_numpy().reshape(-1,1))\n",
    "data_cat = pd.DataFrame(data_cat, columns=[\"Is_\" + str(c) for c in enc.categories_[0]])\n",
    "data_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Please note:* Compared with the examples above, we had to transform the values first.\n",
    "Throwing a whole dataset (as shown in the examples above) at a scaler works well.\n",
    "But when we just want to use the scaler for one column, we have to reshape the values first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column `Job` looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people[\"Job\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we just take the numpy array, we have an array of all values.\n",
    "But the values would be treated as one row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people[\"Job\"].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `reshape(-1,1)` switches the row to a column representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people[\"Job\"].to_numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with this kind of input, the scalers can work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative, we could have also created a `DataFrame` with the one `Series`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distances with Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've learned when working with categorical values, we have to use specific ways to calculate the distances.\n",
    "Using methods from the [`DistanceMetric` class](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html) or own implementations, we can use some clustering algorithms on categorical data as well.\n",
    "But we won't got into details in this exercise.\n",
    "It's primarily an FYI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DENCLUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatelly, there is no implementation of DENCLUE provided by any package in Anaconda or other python packages.\n",
    "Therefore, we had to find another way to get a DENCLUE algorithm up and running.\n",
    "Luckily for us, there is an [open-source implementation](https://github.com/mgarrett57/DENCLUE) available which we can use.\n",
    "\n",
    "**Disclaimer:** While testing the implementation, I ran into several issues with the code.\n",
    "The algorithm was implemented in April 2017, and since then, a module and methods the algorithm uses were changed.\n",
    "Thus, I had to fix the implementation so it runs with the current version of the `networkx` module.\n",
    "It works now, but I cannot guarantee that no other problems will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `networkx` is also not a standard module, we have to install it first with `conda install networkx` or `pip install networkx`.\n",
    "At the time of this exercise, in `conda` version `2.7.1` was available.\n",
    "`pip` offered version `2.8`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to see how it's implemented, feel free to check out the [code in denclue.py](./denclue.py) yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from denclue import DENCLUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can simply use the clustering algorithm by calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DENCLUE()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm offers some hyperparameters as well (all are optional):\n",
    "- **h**: Hill-climbing parameter (you can define the size of the neighborhood)\n",
    "- **eps**: Convergence threshold for density (you can stop hill-climbing at a certain level)\n",
    "- **min_density**: Threshold to consider a cluster and not discard it as noise\n",
    "- **metric**: Distance metric used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the algorithms from sklearn, the interface for this algortihm is limited.\n",
    "There is only a `fit()` method that we can use, the assigned clusters are stored in the `label_` property and we get information on all clusters by calling `clust_info_`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Demo_3Cluster_Noise.csv\", sep=\";\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "data.plot.scatter(\"x\", \"y\", c=\"b\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we have 3 clusters in here, but there are points that aren't that close to an obvious cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's see what the DENCLUE algorithm can do with such data.\n",
    "*Note:* The execution might take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DENCLUE()\n",
    "model.fit(data.to_numpy()) # Unfortunately, the algorithm cannot handle DataFrames - so we need to provide an array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `labels_`, we get the cluster numbers assigned to each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And with `clust_info_`, we get some more insights how the clusters are composed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clust_info_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that the algorithm found a total of 7 clusters in the given data.\n",
    "But we see also, that 4 of the 7 cluster only contain 1 or 2 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we head into plotting, let's introduce a helper method.\n",
    "This method creates a new dataset containing the centroids of each cluster.\n",
    "And we can filter the clusters by specifying a `min_density`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centroids(model, min_density=0.0):\n",
    "    centroids = pd.DataFrame(columns=[\"x\", \"y\", \"density\"])\n",
    "    for i in range(len(model.clust_info_)):\n",
    "        clust = model.clust_info_[i]\n",
    "        if(clust[\"density\"] < min_density):\n",
    "            continue\n",
    "        centroid = pd.DataFrame([clust[\"centroid\"]], columns=[\"x\", \"y\"])\n",
    "        centroid[\"density\"] = clust[\"density\"]\n",
    "        centroids = pd.concat([centroids, centroid], ignore_index=True)\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = get_centroids(model)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "data.plot.scatter(\"x\", \"y\", ax=ax, c=model.labels_, cmap=\"rainbow\", colorbar=False)\n",
    "centroids.plot.scatter(\"x\", \"y\", ax=ax, c=\"k\", s=200, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling the `set_minimum_density`, we can change the number of clusters found in the data.\n",
    "Those clusters not fulfilling the density requirement count as outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_minimum_density(0.01)\n",
    "centroids = get_centroids(model, min_density=0.01)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "data.plot.scatter(\"x\", \"y\", ax=ax, c=model.labels_, cmap=\"rainbow\", colorbar=False)\n",
    "centroids.plot.scatter(\"x\", \"y\", ax=ax, c=\"k\", s=200, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are honest, although some outliers were detected, the clusters still have some points that are quite far away from the center and could also be counted as outliers.\n",
    "To reduce the cluster size, we can limit the boundaries of a cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lim = DENCLUE(h=.5, min_density=0.01)\n",
    "model_lim.fit(data.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model_lim.clust_info_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_lim = get_centroids(model_lim, min_density=0.01)\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "data.plot.scatter(\"x\", \"y\", ax=ax, c=model_lim.labels_, cmap=\"rainbow\", colorbar=False)\n",
    "centroids.plot.scatter(\"x\", \"y\", ax=ax, c=\"k\", s=200, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clusters detected are now smaller.\n",
    "And the one on the bottom left even got split into two clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex01 - Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to use the scalers and normalizer introduced above.\n",
    "First, load the data from **Ex12_01_Data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you have here are some specs on cars.\n",
    "And you will now scale this data.\n",
    "But before you start, create a new empty dataset with just the name and year since you won't scale these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, use a `MinMaxScaler` with a range of `(-1,1)` to scale the `mpg`.\n",
    "Assign the results to the new dataset you've created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `cylinders`.\n",
    "Use a `MinMaxScaler` again, but this time with a range of `(-2,2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use a `StandardScaler` for the `horsepower`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for the `acceleration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last step, use a `RobustScaler` for `displacement` and `weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you are finished.\n",
    "You've successfully scaled some features into more comparable ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex12_01_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex02 - Simple DENCLUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to use the DENCLUE algorithm in its simplest form.\n",
    "To begin, load **Ex12_02_Data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the clusters.\n",
    "Use the value in the `label` column for coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the `DENCLUE` algorithm for the dataset (use only columns `x` & `y`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cluster were found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the centroids of these clusters.\n",
    "*Hint:* You may use the method defined in the introduction.\n",
    "But feel free to code it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data.\n",
    "Use the clusters assigned by the DENCLUE algorithm for coloring.\n",
    "Plot the centroids as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't limit the cluster density.\n",
    "So, we will do it now to get only our 4 expected clusters.\n",
    "Use the `set_minimum_density` method and use a reasonable value for the density so only the 4 clusters remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data again, with the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations!\n",
    "You have used DENCLUE successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex12_02_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex03 - More DENCLUE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use DENCLUE again.\n",
    "Load **Ex12_03_Data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data so you have an idea with what you are dealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the DENCLUE algorithm again.\n",
    "But this time, specify from the beginning a `min_density` of `0.01`.\n",
    "And set `h=.75`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the centroids for the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cluster were found? - Only count those with a density equal or greater than specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data with the found clusters.\n",
    "Also plot the centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you see, clusters were found, but not the number we expected.\n",
    "You know from the original data that we expect 6 clusters.\n",
    "Try to find good values for the parameters to get close to these 6 clusters.\n",
    "This exercise has no right or wrong answer.\n",
    "It shows how hard it can be to find good parameters.\n",
    "\n",
    "A good starting point is the following model:\n",
    "```python\n",
    "DENCLUE(h=.11, eps=.005, min_density=0.1)\n",
    "```\n",
    "But there is space to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex12_03_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex04 - Airbnb Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to find clusters of Airbnb listings in Zurich.\n",
    "In the dataset **Ex12_04_Data.csv**, you'll find the raw data from Zurich published by Airbnb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot these listings in a scatter plot using `longitude` and `latitude` and use the `availability_365` for coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select all listings that are *Entire home/apt* (`room_type`) and are at least availbale *300* days a year (`availability_365`).\n",
    "How many listings would that be?\n",
    "\n",
    "**Note:** During the rest of the exercise, we will just use this reduced dataset.\n",
    "Working with the full dataset would result in a quite long execution of the *DENCLUE* algorithm.\n",
    "And you want to finish this exercise, eventually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and run the *DENCLUE* algorithm with `h=0.003`.\n",
    "This code will take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `minimum_density` to *300*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the centroids of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data again, but with\n",
    "- Outliers should be grayed out (visible as outliers)\n",
    "- Clusters\n",
    "- Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex12_04_Sol.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
