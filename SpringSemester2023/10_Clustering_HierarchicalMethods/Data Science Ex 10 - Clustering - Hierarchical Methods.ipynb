{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Ex 10 - Clustering (Hierarchical Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "03.05.2022, Lukas Kretschmar (lukas.kretschmar@ost.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have some Fun with Hierarchical Clustering approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to have a look at hierarchical clustering approaches and how you can visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Methods (Agglomerative Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to have a look on how you can work with hierarchical clustering.\n",
    "In this section, we will work with an agglomerative clustering algorithm.\n",
    "\n",
    "But let's start with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Demo_WholesaleCustomers.csv\", sep=\";\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset contains information on customers.\n",
    "More precise, we know how much a customer has spent on different segments of food.\n",
    "\n",
    "Although, the values use the same unit (money), they are in different ranges and `Channel` and `Region` represent categorical data.\n",
    "Thus, we need to normalize them first.\n",
    "Here, calling `normalize()` does a bunch of things and we won't go into details.\n",
    "But we just have to know that values per column are normalized to unit norm (are in the range from `0` to `1` compared to all values in the given dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.columns.drop([\"Channel\", \"Region\"]).values\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = normalize(data[features])\n",
    "data_n = data[[\"Channel\", \"Region\"]].copy()\n",
    "data_n[features] = pd.DataFrame(normalized, columns=features)\n",
    "data_n.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can move on and apply a clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dendrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference (Linkage): https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html \\\n",
    "Reference (Dendrogram): https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we do, is checking how many cluster would make sense.\n",
    "For this, we draw a *dendrogram*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = linkage(data_n[features], method=\"ward\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "dendrogram(graph, ax=ax)\n",
    "ax.set(title=\"Dendrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible approach to find a good number of clusters is to locate the longest vertical line not interrupted by a split (this means the best reduction in distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "dend = dendrogram(graph, ax=ax)\n",
    "\n",
    "mark = Ellipse((2625, 8.5), 100, 8, lw=2, ls=\"--\", color=\"r\", fill=False) # the position of the ellipse is just a guess\n",
    "ax.add_artist(mark)\n",
    "\n",
    "ax.axhline(6, c=\"r\", ls=\"--\", lw=2)\n",
    "ax.set(title=\"Dendrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we assume that we can split the data into 2 clusters.\n",
    "\n",
    "But this is just a suggestion, we could also have said that we want to create 3 or 4 clusters as these numbers also would have made sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building clusters with the `AgglomerativeClustering` algorithm works the same way as you've seen with other clustering algorithms.\n",
    "Just call the `fit_predict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AgglomerativeClustering(n_clusters=2)\n",
    "data_n[\"Cluster\"] = model.fit_predict(data_n[features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = len(features)\n",
    "fig, ax = plt.subplots(n_features, n_features, figsize=(30,30))\n",
    "\n",
    "for x in range(n_features):\n",
    "    for y in range(n_features):\n",
    "        data_n.plot.scatter(ax=ax[x,y], x=features[x], y=features[y], c=\"Cluster\", cmap=\"rainbow\", colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://seaborn.pydata.org/generated/seaborn.pairplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above might be a bit uninteresting to write, and there exists already a solution to simplify that.\n",
    "Seaborn offers a simple method, called `pairplot()`, that does the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use the `pairplot()` to color our clusters (`hue`) as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data_n, hue=\"Cluster\", palette=\"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can also select/limit the features you want to show (`vars`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Cluster\"] = data_n[\"Cluster\"]\n",
    "sns.pairplot(data, hue=\"Cluster\", palette=\"rainbow\", vars=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the `AgglomerativeClustering` model, we've just used the default values for the hyperparameters.\n",
    "\n",
    "There are some interesting hyperparameters that we could have changed and/or that could be changed in other use cases:\n",
    "- **affinity**: the distance metric used (default: `eucledian`)\n",
    "  - `eucledian`\n",
    "  - `l1`\n",
    "  - `l2`\n",
    "  - `manhattan`\n",
    "  - `cosine`\n",
    "  - `precomputed`\n",
    "- **linkage**: which distance should be used (default: `ward`)\n",
    "  - `ward` (minimize variance of clusters)\n",
    "  - `average` (average of all distances between points)\n",
    "  - `complete` (maximum distance between two clusters)\n",
    "  - `single` (minimum distance between two clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex01 - Clustering McDonald's Menus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to build clusters with menus from McDonald's.\n",
    "First, load **Ex10_01_Data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you have detailed information on nutritional values of their offers that you will use for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the feature values and store them in a new dataset.\n",
    "Features are all columns besides `Category`, `Item`, `Serving Size`, `Calories`, and `Calories from Fat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the dendrogram of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many cluster should you build?\n",
    "Draw the horizontal line to indicate where to cut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd suggest, you should build 3 clusters.\n",
    "Use the agglomerative clustering algorithm and predict the clusters for your data.\n",
    "Assign these clusters directly to the original dataset that you've loaded at the beginning of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the assigned clusters per category as a bar chart.\n",
    "What do you think what the cluster mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot, we could assume that the clusters distinguish between food and beverages.\n",
    "Not perfectly, but it points into that direction.\n",
    "Some further investigation into data is probably needed.\n",
    "\n",
    "Thus, list the mean and median values per cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the combination of features (as seen in the introduction) to see if you can get a good visualization how these clusters could be interpreted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick two plots that look interesting to you and show them side-by-side and bigger.\n",
    "Use the calories for the size of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the three clusters were built around products with either high amounts of sugar or fat or those in between.\n",
    "Did you find any other meanings for the clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex10_01_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex02 - Clustering Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you do a cluster analysis for movies.\n",
    "You find your data in **Ex10_02_Data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, you have scores and financial information for some movies between 2007 and 2013."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new dataset that does not contain the `Movie` and `Year` columns.\n",
    "And normalize the data of this dataset with the following subsets (normalization per subset):\n",
    "- `RottenTomatoes`, `AudienceScore`\n",
    "- `TheatersOpenWeek`, `OpeningWeekend`, `BOAvgOpenWeekend`,\n",
    "- `DomesticGross`, `ForeignGross`, `WorldGross`, `Budget`, `OpenProfit`\n",
    "- `Profitability`\n",
    "\n",
    "Use the `normalize()` for all groups but `Profitability`.\n",
    "For `Profitability` import the `MinMaxScaler` class from `sklearn.preprocessing` and call the `fit_transform()` method for the data in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw a dendrogram of this new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to build 3 clusters.\n",
    "Where (height) should we make the split?\n",
    "Draw a red line to show the 3 clusters in the dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the agglomerative clustering algorithm model and predict the clusters for the given data.\n",
    "Assign the clusters to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a lot of features and we cannot show all of them in one simple plot, let's plot all combinations of features with the pair plot.\n",
    "Use the cluster for coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, for some feature combinations you can actually see \"good\" clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex10_02_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [As Reference] Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: PCA is not relevant within this course.\n",
    "I've just added a section so you know how to apply it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference (StandardScaler): https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html \\\n",
    "Reference (PCA): https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already heard of PCA in lecture 4.\n",
    "The idea of the Principle Component Analysis - in short PCA - is to calculate a defined number of new attributes called principal components that explain the variance in your data.\n",
    "Having these attributes, we can reduce the dimension of our data, but still hold the contained information to a large part.\n",
    "Depending on the reduction we can make, we may be able to bring n-dimensions down to 2 or 3.\n",
    "And as a result, are able to visualize the data.\n",
    "\n",
    "The theory behind PCA is out of scope for this exercise, but we need to know this handy tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./Demo_Credit.csv\", sep=\";\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run a PCA, we have to prepare the data for the PCA.\n",
    "This means, we have to use a `StandardScaler` to emphasize variables with a high variance and shift all values into the same range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "pca_data = pd.DataFrame(data_scaled, columns=data.columns)\n",
    "pca_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run a PCA on our dataset.\n",
    "As usual, there is a `fit()` method to run the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(pca_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having completed the analysis, we can check how much each principal component adds to the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.around(pca.explained_variance_ratio_, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = range(1, len(pca_data.columns) + 1)\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.plot(n, pca.explained_variance_ratio_.cumsum(), \"o--\")\n",
    "ax.set(ylim=(-.1,1.1), xlabel=\"Number of Components\", ylabel=\"Explained Variance (Cumulative)\", title=\"PCA Variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with 7 components we can explain roughly 95% of the dataset.\n",
    "This means, from our 10 features at the beginning, we were able to break them down to 7 but still keep the information in the data.\n",
    "\n",
    "Depending on the dataset, it's sometimes also possible to break it down to 2 or 3 principal components.\n",
    "And then we are in the range of visualizing the data.\n",
    "But let's do that anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to run the analysis again.\n",
    "But this time, we specify the expected number of components we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=7)\n",
    "pca_data_trans = pca.fit_transform(pca_data)\n",
    "pca_data_trans[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can pack these points into a `DataFrame` and add the known gender again for coloring purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = pd.DataFrame(pca_data_trans, columns=[\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\", \"PC6\", \"PC7\"])\n",
    "data_p[\"Gender\"] = data[\"Gender\"]\n",
    "data_p.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2D plot looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "data_p.plot.scatter(\"PC1\", \"PC2\", ax=ax, c=\"Gender\", cmap=\"rainbow\")\n",
    "ax.set(xlabel=\"PC1\", ylabel=\"PC2\", title=\"Credit PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And based on what we have learned in the last exercise, we can also show 3 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig, ax = plt.subplots(figsize=(10,10), subplot_kw={\"projection\": \"3d\"})\n",
    "ax.scatter(data_p[\"PC1\"], data_p[\"PC2\"], data_p[\"PC3\"], c=data_p[\"Gender\"], cmap=\"rainbow\")\n",
    "ax.set(xlabel=\"PC1\", ylabel=\"PC2\", zlabel=\"PC3\", title=\"Credit PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, we haven't done any clustering by now.\n",
    "\n",
    "Now, we could find a clustering algorithm that can work on this data (but that is not in the scope of this exercise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex - PCA with Income Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you'll run a PCA for a given dataset containing income and personal data.\n",
    "So, load **Ex10_PCA_Data.csv**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you need to run a PCA, you have to scale the data with a `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a PCA and show how much each component contributes to the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the cumulative sum of the variance for each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have seen that 3 components can explain 95% of the total variance.\n",
    "Let's do the analysis again, but this time just with 2 components (we don't need more to visualize the data in 2D).\n",
    "The result should be a new dataset (`DataFrame`) containing the 2 components and the loaded data (4 columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the components as a scatter plot 4 times (2x2) and use the other columns for coloring (1 column per plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex10_PCA_Sol.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
