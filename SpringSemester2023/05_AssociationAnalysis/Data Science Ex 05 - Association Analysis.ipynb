{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Ex 05 - Association Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19.03.2023, Lukas Kretschmar (lukas.kretschmar@ost.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have some Fun with Market Basket Analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you are going to build, find frequent itemsets, and define rules which products are entangled with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've learned, data for the market basket analysis must be in a specific structure (columns are products, rows are baskets, and you have 0/1 or `bool` as values).\n",
    "If not, you need to restructure it.\n",
    "The necessary preprocessing steps are shown at the end of the introduction.\n",
    "\n",
    "But for the introduction, let's assume that we already have data in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mba = pd.read_csv(\"./Demo_MarketBasket.csv\", sep=\";\")\n",
    "print(f\"Number of receipts: {len(mba)}\")\n",
    "mba.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To limit the file size, we stored just `0` nad `1` in the file.\n",
    "But the following algorithms expect `bool` values.\n",
    "So we have to preprocess the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mba = mba.astype(bool)\n",
    "mba.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we don't do this step, the algorithm will still work, but shows a warning that in the future this behavior might no longer work.\n",
    "Thus, it's always a good practice to keep your code as future-proof as possible, and remove warnings whenever possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The needed algorithms are part of the *mlxtend* module.\n",
    "With the following lines, we just import the algorithms that we need for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anaconda does not contain the `mlxtend` module by default and you have to download it with the following command in the `conda-prompt`.\n",
    "*Note:* Depending on how and where you've installed *Anaconda*, you might need *elevated privileges* to install the module.\n",
    "\n",
    "```\n",
    "conda install -c conda-forge mlxtend\n",
    "```\n",
    "\n",
    "If you work with VS Code, then the command is\n",
    "\n",
    "```\n",
    "pip install mlxtend\n",
    "```\n",
    "\n",
    "And then you can import the methods from the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Compared to the imports we did upto now, the call above just imports two functions but not the whole module (as we do with `Numpy` and `Pandas`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating FI Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our frequent itemsets by calling the `apriori()` algorithm that we imported in the line above.\n",
    "Some of the parameters offered by the algorithm are listed below.\n",
    "All are optional besides the first one - we need data, obviously:\n",
    "- `df` : `DataFrame` containing the data\n",
    "- `min_support` (default: `0.5`) : Minimal support the items have to fulfill to be included in the result\n",
    "- `use_colnames` (default: `False`) : Column names are used instead of their indices in the itemsets\n",
    "- `max_len` (default: `None`) : Maximum number of itemsets returned\n",
    "\n",
    "There are some more, but they are not relevant for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_sets = apriori(mba, min_support=.05, use_colnames=True)[[\"itemsets\", \"support\"]]\n",
    "fi_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having our frequent itemsets, we can now build the association rules.\n",
    "The `associatin_rules()` method offers the following parameters:\n",
    "- `df` : `DataFrame` containing the frequent itemsets\n",
    "- `metric` (default: `\"confidence\"`) : Metric used to define if a rule is interesting. Possible values are:\n",
    "    - `\"support\"`\n",
    "    - `\"confidence\"`\n",
    "    - `\"lift\"`\n",
    "    - `\"leverage\"`\n",
    "    - `\"conviction\"`\n",
    "- `min_threshold` (default: `0.8`) : Threashold the metric has to exceed to be considered interesting\n",
    "- `support_only` (default: `False`) : Only computes support, and sets `metric` to \"support\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(fi_sets, metric=\"confidence\", min_threshold=.2)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `antecedents` column, we find the left side of the rule (what was bought) and the `consequents` column contains the right side (what was also bought).\n",
    "And since the rules are a `DataFrame`, we can filter, group and visualize the data with the techniques introduced in the last couple of weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[(rules[\"confidence\"] >= .3) & (rules[\"lift\"] >= 1.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that when milk was bought, vegetables or yogurt were likely also present in the basket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possibility is using visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "rules.plot.scatter(ax=ax, x=\"confidence\", y=\"lift\", c=\"support\", cmap=\"rainbow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end, it's up to the data scientist to read and interpret the data properly, since domain-knowledge is a vital part of the analysing process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But with `apriori()` and `association_rules()` you are able to create frequent itemsets and find rules for products (or things) that are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nice2Know: Restructuring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While researching this topic, I noticed that there is no information available on how you can preprocess your data to fit the algorithm's needs.\n",
    "Since preprocessing steps highly depend on the given data, it's no surprise there is no single unique way to do so.\n",
    "\n",
    "But I included here a step by step approach how you can change the data structure usually found to a format the `apriori()` algorithm can work with.\n",
    "The algorithm expects a table formated like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niceCarts = pd.read_csv(\"./Demo_MarketBasket.csv\", sep=\";\")\n",
    "niceCarts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a cart per row, all possible products are columns and we just have `0` and `1` indicating if the product was present in the specific cart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And usually, data is available in a format like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "carts = pd.read_csv(\"./Demo_Groceries_Unstructured.csv\")\n",
    "carts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we already have a row per shopping cart/receipt - which is what we want.\n",
    "But we have many missing entries since the products are just listed in the cells and the number of columns needs to cover the largest cart.\n",
    "In our case, we have somewhere a cart that contains 32 items.\n",
    "\n",
    "Compared to the table from above, the given structure is not suitable for the algorithm.\n",
    "And we have to restructure the data so it can be used with the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Let's get rid of the *Item(s)* column, since we do not need it.\n",
    "But we store its content to validate our new structure at the end.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oldSums = carts[\"Item(s)\"]\n",
    "carts = carts.drop(\"Item(s)\", axis=\"columns\")\n",
    "carts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Then we stack our data.\n",
    "As a result, we get a multi-index series containing every cart's item in a column.\n",
    "We do this step to get rid of all the `NaN` entries in the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedCarts = carts.stack()\n",
    "stackedCarts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **By reseting the index, we get back a `DataFrame` containing the cart (level_0) and item (0).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedCarts = stackedCarts.reset_index()\n",
    "stackedCarts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **To introduce some readability, we change the column names and drop the obsolete column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedCarts = stackedCarts.rename(columns={\"level_0\" : \"Cart\", 0 : \"Item\"}).drop(\"level_1\", axis=\"columns\")\n",
    "stackedCarts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Sometimes, you already get data in a format like shown above.\n",
    "So, the steps before are not necessary while preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **We now add a new column, indicating that an item is in a cart.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please note:** This step is only necessary when there is no amount information available.\n",
    "Depending on the dataset you have, it's possible that there is already an amount assigned per item.\n",
    "And then you don't need to add the numbers by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackedCarts[\"Amount\"] = 1\n",
    "stackedCarts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Now we create groups by *Cart* and *Item*, select the *Amount* and sum it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call `sum()` because we need to use an aggregate function on our group.\n",
    "Since we don't have any duplicated products per cart, we will just calculate the sum of `1`, which will be `1` in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCarts = stackedCarts.groupby([\"Cart\", \"Item\"])[\"Amount\"].sum()\n",
    "newCarts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure has quite a resemblance to the structure we had after step 2 where we stacked the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **We now do the reverse (compared to above) by unstacking our new data structure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCarts = newCarts.unstack()\n",
    "newCarts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as a result, all our products are now columns containing a `1` where we assigned it and `NaN` in all the other cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **To finalize the dataset, we just have to replace the missing values with 0.\n",
    "And then change the type to `bool`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FYI:** Sometimes the information indicating that an item was present is greater than 1.\n",
    "In this case, you simply change the type first to `bool`.\n",
    "Enforcing `bool` will assign `True` to every value other than `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCarts = newCarts.fillna(0).astype(bool)\n",
    "newCarts.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's about it.\n",
    "\n",
    "We now just have to check that our values are still matching the original data.\n",
    "An easy check is the number of items per cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSums = newCarts.sum(axis=\"columns\")\n",
    "newSums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(newSums - oldSums).any() # is there any row where the calculated sum in newSums is not the same as what was present in carts[\"Item(s)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`False` indicates that there are no differences.\n",
    "For us, this is a good enough indicator that we didn't change the information contained in the data at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. **[Optional]: When you want to store the dataset, you can decrease the size by using `int` instead of `bool` values.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newCartsToStore = newCarts.astype(int)\n",
    "newCartsToStore.to_csv(\"./Intro_Carts.csv\", sep=\";\")\n",
    "newCartsToStore.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex01 - Apriori on a simple Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file **Ex05_01_Data.csv** and list the first 5 lines.\n",
    "You may have to specify the separator `sep` used to get a correct `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should already be familiar with this data.\n",
    "But as you can see, some values are missing and there are `X` instead of `1`.\n",
    "Correct this and show the first 5 lines again.\n",
    "Don't forget to use `bool` values at the end in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the frequent itemset with a `min_support >= .3`.\n",
    "And show the whole itemset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the rules that have a `confidence >= .85` and list them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex05_01_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex02 - Apriori on a large Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file **Ex05_02_Data.csv** and list the first couple of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many shopping carts are in the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the frequent itemsets with a `min_support >= .3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the rules with `confidence >= .5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rules did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort the rules by their `lift` with the most promising rule on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex05_02_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex03 - Processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file **Ex05_03_Data.csv** and show the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data is not structured the way we need it to use it with the `apriori()` function.\n",
    "It's now your job to process the data.\n",
    "The goal is that you can use the result of this exercise as the input for the next one (don't worry, the next exercise comes with its own input data if you cannot accomplish this one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the data is structured like **step 5** in the introduction.\n",
    "Thus, just need to apply **steps 6 - 8**.\n",
    "But compared to the intro, you now have also to include the *Country* into the grouping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the example in the introduction, we have here a multi-index.\n",
    "But that shouldn't bother us.\n",
    "It's actually quite useful when filtering for one country."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can store your `DataFrame` with `to_csv(filename,sep)`[(Reference)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html) and reload the file in the next exercise.\n",
    "If you save your data, it could be handy to replace every `0` with `np.nan` again or just don't do *step 8*.\n",
    "This way, the output file needs 50% less storage capacity.\n",
    "\n",
    "Or you can simple reuse the variable containing your `DataFrme` in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for storing file comes here - if you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex05_03_Sol.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex04 - Apriori on your Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that you use the processed data from the previous exercise.\n",
    "If you were not able to complete the preprocessing, you can use **Ex05_04_Data.csv**.\n",
    "This file contains the same data that you would have gotten from the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start this exercise from a file (the provided one or the one you created in the previous exercise), you need to do the following steps.\n",
    "Otherwise, you can skip these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the content of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Set *Country* and *InvoiceNo* as index with `set_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Fill the missing data (basically do step 8 from the introdcution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From here, the exercise is the same - for those starting from a file or taking the variable from the exercise above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will have a look at the information from *Switzerland*.\n",
    "Thus, just select the invoices from `Switzerland` by using `<data>.loc[\"Switzerland\"]` (`<data>` is a placeholder for your variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many invocies are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the frequent itemsets with a `min_support >= .1`.\n",
    "And show all the rules with a `confidence >= .7` and order them by their `lift` (best on top)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many rules did you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these retail information are from a UK-based online store, there are much more invoivces from the UK.\n",
    "So get them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many invoices from the UK are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's find the frequent itemsets for those (use the same parameters from above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any frequent itemsets? - Usually, the larger you datasets are, the lower you have to set the support threshold to even get results.\n",
    "Thus, lower the `min_support` to `.03` and build the rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are we surprised that these rules were found?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ./Ex05_04_Sol.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
